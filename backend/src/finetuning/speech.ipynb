{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "src_path = Path(os.getcwd()).parent.parent.joinpath('src').absolute()\n",
    "sys.path.append(str(src_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flok3n/develop/konrads/backend/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingsound import TrainingArguments, ModelArguments, SpeechRecognitionModel, TokenSet\n",
    "from typing import NamedTuple, TypedDict\n",
    "import numpy as np\n",
    "from persistence.model import FileMetadata\n",
    "from pathlib import Path\n",
    "from persistence.db import Database\n",
    "from persistence.file_metadata_repository import FileMetadataRepository\n",
    "from features.transcriber import Transcriber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = Path('/home/flok3n/konrad'); DB_DIR = ROOT_DIR\n",
    "db = Database(DB_DIR)\n",
    "await db.init_db()\n",
    "\n",
    "repo = FileMetadataRepository(db)\n",
    "\n",
    "files = await repo.load_all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_with_fixed_transcript = [f for f in files if f.is_transcript_analyzed and f.is_transcript_fixed]\n",
    "len(files_with_fixed_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def preprocess_data(files: list[FileMetadata]):\n",
    "    transcriber = Transcriber(None)\n",
    "    for f in files:\n",
    "        src_path = ROOT_DIR.joinpath(f.name)\n",
    "        target_path = PREPROCESSED_DATA_DIR.joinpath(f.name + '.wav')\n",
    "        file_bytes = await transcriber._get_preprocessed_audio_file(src_path)\n",
    "        with open(target_path, 'wb') as target:\n",
    "            file_bytes.seek(0)\n",
    "            target.write(file_bytes.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# await preprocess_data(files_with_fixed_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "class TrainItem(TypedDict):\n",
    "    path: str\n",
    "    transcription: str\n",
    "\n",
    "class Dataset(NamedTuple):\n",
    "    train: list[TrainItem]\n",
    "    eval: list[TrainItem]\n",
    "\n",
    "\n",
    "def get_train_eval_dataset(files: list[FileMetadata], eval_split=0.2) -> Dataset:\n",
    "    items_with_not_empty_trancript = [f for f in files if f.transcript != '']\n",
    "    items_with_empty_trancript = [f for f in files if f.transcript == '']\n",
    "\n",
    "    assert len(items_with_empty_trancript) > 1 and len(items_with_not_empty_trancript) > 1\n",
    "\n",
    "    train, eval = [], [] \n",
    "    for src in (items_with_not_empty_trancript, items_with_empty_trancript):\n",
    "        items = [TrainItem(path=str(PREPROCESSED_DATA_DIR.joinpath(f.name + '.wav').absolute()), transcription=str(f.transcript)) for f in src]\n",
    "        num_eval_items = max(1, int(eval_split * len(items)))\n",
    "        eval_idxs = set(np.random.choice(range(len(items)), num_eval_items, replace=False))\n",
    "        for i, item in enumerate(items):\n",
    "            if i in eval_idxs:\n",
    "                eval.append(item)\n",
    "            else:\n",
    "                train.append(item)\n",
    "\n",
    "    np.random.shuffle(train)\n",
    "    np.random.shuffle(eval)\n",
    "    return Dataset(train=train, eval=eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_train_eval_dataset(files_with_fixed_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingsound import GreedyDecoder, Decoder\n",
    "import torch\n",
    "\n",
    "class DecoderAdapter(Decoder):\n",
    "    def __init__(self, token_set, skip_special_tokens = True, ms_per_timestep = 20, probability_offset = 1, save_for_dev = True):\n",
    "        super().__init__(token_set, skip_special_tokens, ms_per_timestep, probability_offset)\n",
    "        self.save_for_dev = save_for_dev\n",
    "\n",
    "    def _get_predictions(self, logits: torch.Tensor):\n",
    "\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        predictions = self._ctc_decode(predicted_ids)\n",
    "\n",
    "        if self.save_for_dev:\n",
    "            with open('./logits.np', 'wb') as f:\n",
    "                logits_np = logits.detach().cpu().numpy()\n",
    "                np.save(f, logits_np)\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_dev_logits() -> np.ndarray:\n",
    "        with open('./logits.np', 'rb') as f:\n",
    "            return np.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trie:\n",
    "    class Node:\n",
    "        def __init__(self, num_tokens: int, is_terminal: bool):\n",
    "            self.children = [None] * num_tokens\n",
    "            self.is_terminal = is_terminal\n",
    "\n",
    "    def __init__(self, num_tokens: int):\n",
    "        self.num_tokens = num_tokens\n",
    "        self.root = self.Node(num_tokens=num_tokens, is_terminal=False)\n",
    "\n",
    "    def add(self, word_token_ids: list[int]):\n",
    "        if not word_token_ids:\n",
    "            return\n",
    "        cur = self.root\n",
    "        for token in word_token_ids:\n",
    "            if cur.children[token] is None:\n",
    "                tmp = self.Node(self.num_tokens, is_terminal=False)\n",
    "                cur.children[token] = tmp\n",
    "            cur = cur.children[token]\n",
    "        cur.is_terminal = True\n",
    "\n",
    "    def search(self, word_token_ids: list[int]) -> tuple[bool, int, Node]:\n",
    "        '''Returns: True iff word exists, length of the longest prefix, last Node on the path'''\n",
    "        cur = self.root\n",
    "        for i, token in enumerate(word_token_ids):\n",
    "            if cur.children[token] is None:\n",
    "                return False, i, cur\n",
    "            cur = cur.children[token]\n",
    "        return cur.is_terminal, len(word_token_ids), cur\n",
    "    \n",
    "    def has(self, word_token_ids: list[int]) -> bool:\n",
    "        return self.search(word_token_ids)[0]\n",
    "    \n",
    "    def get_possible_next_tokens(self, node: \"Trie.Node\") -> list[int]:\n",
    "        if node is None:\n",
    "            return []\n",
    "        return [i for i, x in enumerate(node.children) if x is not None] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "import editdistance\n",
    "from collections import deque\n",
    "\n",
    "class BKTree:\n",
    "    class Node:\n",
    "        def __init__(self, word: str):\n",
    "            self.children: dict[int, \"BKTree.Node\"] = {}\n",
    "            self.word = word\n",
    "\n",
    "    def __init__(self, root_word: str):\n",
    "        self.root = self.Node(root_word)\n",
    "\n",
    "    def add(self, word: str):\n",
    "        cur = self.root\n",
    "        while True:\n",
    "            dist = editdistance.eval(word, cur.word)\n",
    "            if dist == 0:\n",
    "                return\n",
    "            child = cur.children.get(dist)\n",
    "            if child is None:\n",
    "                cur.children[dist] = self.Node(word)\n",
    "                break\n",
    "            else:\n",
    "                cur = child\n",
    "\n",
    "    def search(self, word: str, max_distance: int=1) -> Generator[tuple[str, int], None, None]:\n",
    "        queue: deque[\"BKTree.Node\"] = deque()\n",
    "        queue.append(self.root)\n",
    "        while queue:\n",
    "            cur = queue.popleft()\n",
    "            dist = editdistance.eval(word, cur.word)\n",
    "            if dist <= max_distance:\n",
    "                yield cur.word, dist\n",
    "            for d in range(max(dist - max_distance, 0), dist + max_distance + 1):\n",
    "                if next := cur.children.get(d):\n",
    "                    queue.append(next)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Trie(5)\n",
    "t.add([1, 3])\n",
    "t.search([1, 3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkt = BKTree('book')\n",
    "words = ['books', 'cake', 'boo', 'cape', 'boon', 'cook', 'cart']\n",
    "for w in words:\n",
    "    bkt.add(w)\n",
    "\n",
    "for w in bkt.search('name', max_distance=2):\n",
    "    print(w[0], w[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import msgpack\n",
    "import gzip\n",
    "import wordfreq\n",
    "with gzip.open(wordfreq.DATA_PATH.joinpath('large_pl.msgpack.gz'), 'rb') as f:\n",
    "    data = msgpack.load(f, raw=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ó', 'ą', 'ć', 'ę', 'ł', 'ń', 'ś', 'ź', 'ż']\n",
    "diacritics = ['ó', 'ą', 'ć', 'ę', 'ł', 'ń', 'ś', 'ź', 'ż']\n",
    "\n",
    "tm = {x: i for i, x in enumerate(tokens)}\n",
    "\n",
    "def get_token_id(token: str) -> int:\n",
    "    try:\n",
    "        return tm[token]\n",
    "    except:\n",
    "        raise KeyError(f'unknown token: {token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_tokens = set()\n",
    "trie = Trie(num_tokens=len(tokens))\n",
    "for bucket in data[1:]:\n",
    "    for word in bucket:\n",
    "        try:\n",
    "            tokenized_word = [get_token_id(x) for x in word]\n",
    "            trie.add(tokenized_word)\n",
    "        except KeyError as e:\n",
    "            unknown_tokens.add(e.args[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_added_words = 0\n",
    "bkt = BKTree('kurwa')\n",
    "for bucket in data[1:]:\n",
    "    for word in bucket:\n",
    "        try:\n",
    "            tokenized_word = [get_token_id(x) for x in word]\n",
    "            bkt.add(word)\n",
    "            num_added_words += 1\n",
    "            if num_added_words % 10000 == 0:\n",
    "                print(f'\\r{num_added_words}', end='')\n",
    "        except KeyError as e:\n",
    "            unknown_tokens.add(e.args[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictionaryAssistedDecoder(Decoder):\n",
    "    def __init__(self, token_set, dictionary: Trie, edit_distance_search_tree: BKTree, disctionary_token_id_lut: dict[str, int], average_word_length=8):\n",
    "        super().__init__(token_set)\n",
    "        self.dictionary = dictionary\n",
    "        self.edit_distance_search_tree = edit_distance_search_tree\n",
    "        self.disctionary_token_id_lut = disctionary_token_id_lut\n",
    "        self.best_configuration_compute_buff = np.zeros((2 * average_word_length + 1, average_word_length*10), dtype=np.float32)\n",
    "\n",
    "    def _get_predictions(self, logits: torch.Tensor):\n",
    "        assert logits.shape[0] == 1\n",
    "        logits = logits[0, ...]\n",
    "        predicted_ids = []\n",
    "        N = logits.shape[0]\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        log_probs = torch.log(probs)\n",
    "        tokens_sorted_by_prob = probs.argsort(dim=-1, descending=True)\n",
    "        word_start = 0\n",
    "        previous_word = None\n",
    "        while word_start < N:\n",
    "            i = word_start\n",
    "            word_decoding_state: list[int] = []\n",
    "            while i < N:\n",
    "                most_likely_token = tokens_sorted_by_prob[i, 0]\n",
    "                prob = probs[i, most_likely_token]\n",
    "                if len(word_decoding_state) == 0 and (most_likely_token == self.token_set.blank_token_id or\n",
    "                                                      most_likely_token == self.token_set.silence_token_id):\n",
    "                    word_start += 1\n",
    "                    break\n",
    "                if most_likely_token == self.token_set.blank_token_id:\n",
    "                    i += 1\n",
    "                    continue\n",
    "                if most_likely_token == self.token_set.silence_token_id:\n",
    "                    word = self._join_word(word_decoding_state)\n",
    "                    word_exists, existing_prefix_size, last_node = self.dictionary.search(self._convert_to_dictionary_tokens(word_decoding_state))\n",
    "                    if word_exists:\n",
    "                        # greedily accept word\n",
    "                        word_start, previous_word = self._accept(predicted_ids, word_decoding_state, i)\n",
    "                        break\n",
    "                    elif previous_word is not None:\n",
    "                        combined_word = previous_word + word_decoding_state\n",
    "                        if self.dictionary.has(self._convert_to_dictionary_tokens(combined_word)):\n",
    "                            predicted_ids.pop()\n",
    "                            word_start, previous_word = self._accept(predicted_ids, word_decoding_state, i)\n",
    "                            break\n",
    "                    if prob < 0.9 and existing_prefix_size == len(word): \n",
    "                        # maybe this isn't the end of word, lookup trie if we have some options and if after this blank there is likely some letter\n",
    "                        if possible_next_tokens := self.dictionary.get_possible_next_tokens(last_node):\n",
    "                            LOOK_AHEAD = 5\n",
    "                            should_continue_with_word = False\n",
    "                            for k in range(i, min(i+LOOK_AHEAD, N)):\n",
    "                                most_probable_existing_token = possible_next_tokens[0]\n",
    "                                for token in possible_next_tokens[1:]:\n",
    "                                    if probs[k, token] > probs[k, most_probable_existing_token]:\n",
    "                                        most_probable_existing_token = token\n",
    "                                if probs[k, most_probable_existing_token] > 0.25:\n",
    "                                    word_decoding_state.append(most_probable_existing_token)\n",
    "                                    i = k + 1\n",
    "                                    should_continue_with_word = True\n",
    "                                    break\n",
    "                        if should_continue_with_word:\n",
    "                            continue\n",
    "\n",
    "                    # try splitting into two words and correct them separately                        \n",
    "                    best_split, split_best_log_prob = self._split_word_and_attempt_correcting(\n",
    "                        word_decoding_state, log_probs, word_start, i-1)\n",
    "\n",
    "                    # find existing word with small levenshtein distance and highest probability\n",
    "                    whole_best_alternative_tokens, whole_best_log_prob = self._correct_word(\n",
    "                        log_probs, word, word_start, i-1, max_dist=1 if len(word) <= 3 else 2)\n",
    "                    \n",
    "                    if whole_best_alternative_tokens is None and best_split is None:\n",
    "                        self._accept(predicted_ids, word_decoding_state, i)\n",
    "                        word_start, previous_word = self._accept(predicted_ids, whole_best_alternative_tokens, i)\n",
    "                    else:\n",
    "                        if split_best_log_prob is not None and (whole_best_log_prob is None or split_best_log_prob > whole_best_log_prob):\n",
    "                            self._accept(predicted_ids, best_split[0], i)\n",
    "                            word_start, previous_word = self._accept(predicted_ids, best_split[1], i)\n",
    "                        else:\n",
    "                            word_start, previous_word = self._accept(predicted_ids, whole_best_alternative_tokens, i)\n",
    "                    break\n",
    "                            \n",
    "                if len(word_decoding_state) == 0 or most_likely_token != tokens_sorted_by_prob[i-1, 0]:\n",
    "                    word_decoding_state.append(most_likely_token)\n",
    "                i += 1\n",
    "\n",
    "        predictions = self._ctc_decode([predicted_ids])\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "    def _split_word_and_attempt_correcting(self, word_decoding_state: list[int],\n",
    "            log_probs: torch.Tensor, word_start: int, word_end: int) -> tuple[tuple[list[int], list[int]] | None, float]:\n",
    "        if len(word_decoding_state) < 3:\n",
    "            return None, -np.inf\n",
    "\n",
    "        best_split, best_log_prob = None, None\n",
    "\n",
    "        for split_pos in range(2, len(word_decoding_state) - 2):\n",
    "            first_tokens, second_tokens = word_decoding_state[:split_pos], word_decoding_state[split_pos:]\n",
    "            first_needs_correction, second_needs_correction = False, False\n",
    "            if not self.dictionary.has(self._convert_to_dictionary_tokens(first_tokens)):\n",
    "                first_needs_correction = True\n",
    "            if not self.dictionary.has(self._convert_to_dictionary_tokens(second_tokens)):\n",
    "                second_needs_correction = True\n",
    "\n",
    "            partial_log_prob = 0.\n",
    "            if first_needs_correction:\n",
    "                first_tokens, lp = self._correct_word(log_probs, self._join_word(first_tokens), word_start, word_start + split_pos - 1, max_dist=1) \n",
    "                if lp is None:\n",
    "                    continue\n",
    "                partial_log_prob += lp\n",
    "            else:\n",
    "                partial_log_prob += self._get_log_probability_of_best_configuration(log_probs, first_tokens, word_start, word_start + split_pos - 1)\n",
    "            \n",
    "            if second_needs_correction:\n",
    "                second_tokens, lp = self._correct_word(log_probs, self._join_word(second_tokens), word_start + split_pos, word_end, max_dist=1) \n",
    "                if lp is None:\n",
    "                    continue\n",
    "                partial_log_prob += lp\n",
    "            else:\n",
    "                partial_log_prob += self._get_log_probability_of_best_configuration(log_probs, first_tokens, word_start + split_pos, word_end)\n",
    "            \n",
    "            if best_log_prob is None or partial_log_prob > best_log_prob:\n",
    "                best_split, best_log_prob = (first_tokens, second_tokens), partial_log_prob\n",
    "\n",
    "        return best_split, best_log_prob\n",
    "    \n",
    "    def _correct_word(self, log_probs: torch.Tensor, word: str, start_idx: int, end_idx: int, max_dist: int) -> tuple[list[int] | None, float | None]:\n",
    "        alternatives = list(self.edit_distance_search_tree.search(word, max_distance=max_dist))\n",
    "        overall_best_alternative_tokens, overall_best_log_prob = None, None\n",
    "        for dist in range(1, 3):\n",
    "            alternatives_with_dist = [x[0] for x in alternatives if x[1] == dist]\n",
    "            if not alternatives_with_dist:\n",
    "                continue\n",
    "            tokens_of_alternatives = [self._tokenize_word(x) for x in alternatives_with_dist]\n",
    "            most_probable_alternative, best_log_prob = 0, None\n",
    "            for k, tokens in enumerate(tokens_of_alternatives):\n",
    "                lp = self._get_log_probability_of_best_configuration(log_probs, tokens, start_idx, end_idx)\n",
    "                if best_log_prob is None or lp > best_log_prob:\n",
    "                    most_probable_alternative, best_log_prob = k, lp\n",
    "            if best_log_prob is not None:\n",
    "                if overall_best_log_prob is None or best_log_prob > overall_best_log_prob:\n",
    "                    overall_best_alternative_tokens, overall_best_log_prob = tokens_of_alternatives[most_probable_alternative], best_log_prob\n",
    "        return overall_best_alternative_tokens, overall_best_log_prob\n",
    "\n",
    "    def _get_log_probability_of_best_configuration(self, log_probs: torch.Tensor, tokens: list[int], start_idx: int, end_idx: int) -> float:\n",
    "        N = end_idx - start_idx + 1\n",
    "        if len(tokens) > N:\n",
    "            return -np.inf\n",
    "\n",
    "        if self.best_configuration_compute_buff.shape[0] >= len(tokens) + 1 and self.best_configuration_compute_buff.shape[1] >= N + 1:\n",
    "            F = self.best_configuration_compute_buff\n",
    "        else:\n",
    "            F = np.zeros((len(tokens) + 1, N + 1), dtype=np.float32)\n",
    "\n",
    "        for i in range(1, N+1):\n",
    "            F[0, i] = F[0, i-1] + log_probs[start_idx + i - 1, self.token_set.blank_token_id]\n",
    "\n",
    "        for i in range(1, len(tokens) + 1):\n",
    "            for j in range(i, N + 1):\n",
    "                take_cur_lp = log_probs[start_idx + j - 1, tokens[i - 1]]\n",
    "                blank_lp = log_probs[start_idx + j - 1, self.token_set.blank_token_id]\n",
    "                if i == j:\n",
    "                    F[i, j] = F[i-1, j-1] + take_cur_lp\n",
    "                else:\n",
    "                    F[i, j] = max(F[i, j-1] + blank_lp, F[i-1, j-1] + take_cur_lp)\n",
    "        \n",
    "        return F[len(tokens), N]\n",
    "    \n",
    "    def _accept(self, predicted_ids: list[int], token_ids: list[int], i: int) -> tuple[int, list[int]]:\n",
    "        if token_ids:\n",
    "            edited_token_ids = [token_ids[0]]\n",
    "            for token in token_ids[1:]:\n",
    "                if token == edited_token_ids[-1]:\n",
    "                    # ctc decoding filters same consecutive tokens that are not separated by blank\n",
    "                    edited_token_ids.append(self.token_set.blank_token_id)\n",
    "                edited_token_ids.append(token)\n",
    "            predicted_ids.extend(edited_token_ids)\n",
    "        predicted_ids.append(self.token_set.silence_token_id)\n",
    "        return i + 1, token_ids\n",
    "    \n",
    "    def _tokenize_word(self, word: str) -> list[int]:\n",
    "        return [self.token_set.id_by_token[x] for x in word]\n",
    "    \n",
    "    def _convert_to_dictionary_tokens(self, token_ids: list[int]) -> list[int]:\n",
    "        return [x - len(self.token_set.special_tokens) for x in token_ids]\n",
    "    \n",
    "    def _join_word(self, token_ids: list[int]) -> str:\n",
    "        return ''.join([self.token_set.tokens[x] for x in token_ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-polish\", device='cuda')\n",
    "model = SpeechRecognitionModel(str(OUTPUT_DIR), device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = Transcriber(None)\n",
    "engine = tb.Engine(tb, lambda: (model, DecoderAdapter(model.token_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcription = await engine.transcribe(ROOT_DIR.joinpath('317290092_5660838990665621_2090424565105818065_n.mp4'))\n",
    "# transcription = await engine.transcribe(ROOT_DIR.joinpath('video-1591823465.mp4'))\n",
    "transcription = await engine.transcribe(ROOT_DIR.joinpath('353634787_6536568179715415_2948274603283924016_n.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = DecoderAdapter.load_dev_logits()\n",
    "\n",
    "DecoderAdapter(model.token_set, save_for_dev=False)(torch.from_numpy(logits))[0]['transcription']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DictionaryAssistedDecoder(model.token_set, dictionary=trie, edit_distance_search_tree=bkt, disctionary_token_id_lut=tm)(torch.from_numpy(logits))[0]['transcription']\n",
    "# DictionaryAssistedDecoder(model.token_set, dictionary=trie, edit_distance_search_tree=bkt)(torch.from_numpy(logits))[0]['transcription']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "firany_files = []\n",
    "for f in files:\n",
    "    if 'firany' in f.name:\n",
    "        firany_files.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trie.search([get_token_id(x) for x in 'poddawać'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trie.has([get_token_id(x) for x in 'ćme'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_results = []\n",
    "for file in firany_files[1:2]:\n",
    "    transcription = await engine.transcribe(ROOT_DIR.joinpath(file.name), DecoderAdapter(model.token_set))\n",
    "    transcription_results.append(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f, t in zip(firany_files, transcription_results):\n",
    "    print(f'before: {f.transcript}')\n",
    "    print(f'after: {t}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-polish\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=5,\n",
    "    fp16=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned = model.finetune(\n",
    "    str(OUTPUT_DIR),\n",
    "    train_data=[*dataset.train, *dataset.eval],\n",
    "    # eval_data=dataset.eval,\n",
    "    eval_data=None,\n",
    "    token_set=model.token_set,\n",
    "    training_args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir finetuning/models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
